---
editor_options: 
  markdown: 
    wrap: 72
---

# 第五讲：如何清理数据—数据的预处理{#lesson-5}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # Wickham的数据整理的整套工具
pdf.options(height=10/2.54, width=10/2.54, family="GB1") # 注意：此设置要放在最后
```

## **批量读取文件**

在这节课中我们会讲述如何将我们从实验或者问卷中获得的原始数据进行整理，转化为可以用于统计分析的数据，我们有两种方法，分别是for loop方法和lapply函数方法，他们都可以将多个散数据合并为一个完整的数据。
两个方法各有优劣：
for loop方法思维难度低，书写难度高，更好理解，但是代码更长，并且存在中间变量。
lapply方法思维难度高，书写难度低，更难理解，但是代码更简洁，存在"看不见"的局部变量。

在我们的示例数据中，会发现有很多子文件，比如以practice、match和category等结尾的数据，它表示的是三个阶段：practice表示练习阶段，match表示match任务阶段，category表示categorization任务阶段。现在我们需要找出所有包含match且以out结尾的文件，这些才是我们需要读取并合并的数据。

### **通配符**

逐个列出我们需要合并的文件太过于麻烦且低效，我们可以通过list来列出所有文件名，然后根据一定的规则来筛选，这样可以快速得到我们需要合并的文件名。这个时候我们就要用到通配符了。

通配符是一种特殊字符，它可以在匹配文件名或其他文本字符串时代替其他字符。R中常使用的通配符包括"*""?"和"[]"
*：代表任意数量的字符，例如：
*.csv将匹配所有以.csv结尾的文件。
?：代表单个字符，例如file?.txt将匹配file1.txt，file2.txt等文件，但不会匹配file10.txt。
[]：用于匹配指定的一组字符。例如，file[123].txt将匹配file1.txt，file2.txt和file3.txt。

我们要做的第一件事是扫描这个文件夹，把里面所有的文件和文件夹都读取出来。然后我们使用通配符来匹配文件夹里包含特殊信息的文件。也就是说，我们需要根据文件名是否包含match来筛选文件夹里的文件。

我们可能更多的使用*字符，因为如果使用?这个字符的话。file?.txt，能够匹配的符合条件的文件就是file1、file2、file3等含单个字符的文件，当筛选到file10时，就会因为后面有两个字符而不匹配。

中括号里的字符是“或”的关系，就是说，中括号里的123代表file1后面跟1、2或3都可以。这样可以任意灵活地匹配。因为有可能你不知道文件夹里有多少个文件，你就把它们全部写在中括号里，只要它包含在里面，我们就把它读取出来。

```{r}
library(DT)
# 所有路径使用相对路径
library(here)
# 包含了dplyr和%>%等好用的包的集合
library(tidyverse)
# 养成用相对路径的好习惯，便于其他人运行你的代码
WD <-  here::here()
getwd()
```

### for**循环思路**

那么我们该怎么使用它呢？我们需要先加载tidyverse这个包，提醒大家在开始处理之前要load这个包，否则在使用函数时会出错。

```{r}
# 把所有符合某种标题的文件全部读取到一个list中，也就是说，在相对路径"data/match"中抓取文件名包含"data_exp7_rep_match_"的文件
files <- list.files(file.path("data/match"), pattern = "data_exp7_rep_match_.*\\.out$")

head(files, n = 10L)

str(files)
```

我们先使用files来存储我们从这个文件夹里扫描到的所有文件的名字
在R中，list.files()是一个函数，它可以扫描文件夹里的所有文件。第一个参数是你要在哪个文件夹里扫描————即在当前工作目录里的data文件夹中，然后在这个文件夹中的match文件夹里进行扫描。同时，我们使用pattern来筛选文件夹里的文件名，比如包含了"match"的，或者更加精确的，筛选以data_exp7__rep开头的文件。并使用一个通配符来表示筛选所有以out结尾的文件。
函数运行后，我们就得到了所有符合筛选目标的文件名。我们只会得到它们的文件名，不包含它们的路径。如果我们把它们读取出来并列出前面10个文件名，可以看到它们确实完全符合我们筛选的规则。

读取完后，我们可以使用for循环。首先，我们需要创建一个空的列表来存储读取的数据。for循环的结构是for（条件）{内容}。我们使用i来循环，in表示我们要循环的范围。我们首先将i设置为1，然后做某些事情，然后在i等于2时再做同样的事情，一直到i等于10。

```{r df.mt.out.fl}
# 创建一个空的列表来存储读取的数据框
df_list <- list()
# 循环读取每个文件，处理数据并添加到列表中
for (i in seq_along(files)) { # 重复"读取到的.out个数"的次数。根据files的list长度生成数值的sequence。对files里的第一个值执行read.table()，并将结果放进df_list()中，接着对第二个值执行，直至循环结束整个files。
  # 对每个.out，使用read.table
  df <- read.table(file.path("data/match", files[i]), header = TRUE) #read.table似乎比read.csv更聪明，不需要指定分隔符
  # 给读取到的每个.out文件的每个变量统一变量格式
  df <- dplyr::filter(df, Date != "Date") %>% # 因为有些.out文件中部还有变量名，所以需要用filter把这些行过滤掉
    dplyr::mutate(Date = as.character(Date),Prac = as.character(Prac),
                  Sub = as.numeric(Sub),Age = as.numeric(Age),Sex = as.character(Sex),Hand = as.character(Hand),
                  Block = as.numeric(Block),Bin = as.numeric(Bin),Trial = as.numeric(Trial),
                  Shape = as.character(Shape),Label = as.character(Shape),Match = as.character(Match),
                  CorrResp = as.character(CorrResp),Resp = as.character(Resp),
                  ACC = as.numeric(ACC),RT = as.numeric(RT))
  # 将数据框添加到列表中
  df_list[[i]] <- df
}
# 合并所有数据框，只有当变量的属性一致时，才可以bind_rows
# bind_rows 意味着把list中的所有表格整合成一个大表格
df.mt.out.fl <- dplyr::bind_rows(df_list)
# 清除中间变量
rm(df,df_list,files,i)
# 如果你将这个步骤写成函数，则这些变量自然不会出现在全局变量中
```

在for循环中，我们使用df=read.table来读取文件。文件的路径是相对路径，是data match和files的组合。files是文件夹中的一个文件。
当i等于1时，我们读取的是第一个文件。我们按照header为true的方式去读取。
我们发现out文件里保存的数据有些小问题（包含多个title行），因此我们需要去掉重复的行。我们可以用一个规则，即如果data再次出现，表示它重复了上面这个列名，我们就把所有这样的行去掉。
之后，我们还可以用as.character和as.numerical来统一每个变量的变量类型。
然后，把数据放入数据框中。当i等于1时，它就是对第一个文件做了上述操作。然后，我们把它复制到数据列表里面，第一个位置就是第一个out文件，它经过了转换之后的数据框，就装到数据框列表里面的第一个位置。然后，我们通过循环读取id位中的第二个文件，并倒数10个，直到读取完所有10个文件。

将所有数据放入列表后，我们可以使用bind_rows函数将它们合并成一个大的数据框。bind_rows是Dplyr中一个常用的函数，它可以通过行将不同的数据框合并。因此，data列表实际上是一个完整的列表，其中包含一个一个的out文件的数据，经过初步转换后形成的数据框。由于它们的列都是一模一样的，我们可以直接将它们合并成一个完整的数据框。

这就是for循环的逻辑：（1）获取想要读取的文件名；（2）通过迭代的方式依次读取这些文件名，并将它们放入一个数据列表（DataList）中；（3）对这个List进行合并得到完整数据框（Dataframe)。
这个操作思路非常简洁易懂。当然，真正理解for循环的前提是要正确理解它的工作原理。此外，for循环还可以用在很多场景中：
（1）进行重复性操作。
当你需要重复大量做某件事情而没有现成函数可以辅助时，最简单的方法就是写一个for循环来完成。i可以替代很多代码，比如你有五个对象需要做同样的操作，你可以写一个i从1到5的for循环，将每个对象放进去做同样的操作。否则，你就需要不断复制你的代码，然后改变其中的一个值，再运行一遍，这样既费时又费力。
（2）应用于数据的批处理。
如果你经常进行批量处理工作，学会for循环会非常有帮助。它可以帮助我们更方便地读取并合并文件夹中的文件。

最终的结果应该是25920 obs 16 variables

```{r df.mt.out DT, echo=FALSE}
DT::datatable(head(df.mt.out.fl, 100),
              fillContainer = TRUE, options = list(pageLength = 7))
```

### lapply思路

我们可以使用管道操作符“%>%”和lapply这样更方便的工具来代替for循环完成批量合并数据这类常用的操作。这个符号在Dplyr包中
管道符能够将前一步的结果作为下一个函数的输入。我们首先列出符合条件的文件名，并将其作为一个列表输入到lapply函数中。lapply函数是一个apply系列的函数，它将函数应用于一个列表上。
在这里，我们将read.table函数应用于列表中的每个元素，其中文件路径是x的参数，head=true是read.table的参数。这个操作实际上就是用一行命令代替了for循环的操作。

```{r df.mt.raw.la}
# 获取所有的.out文件名
df.mt.out.la <- list.files(file.path("data/match"), pattern = "data_exp7_rep_match_.*\\.out$") %>%
  # 对读取到的所有.out文件x都执行函数read.table（x就是一个代称，指将上一步读取到的所有文件名暂时命名为x，命名为abc也可以。只要保证function()括号内和后面的内容是一致的就可以）
  lapply(.,function(x) read.table(file.path("data/match", x), header = TRUE)) %>% 
  # 对所有被read.table处理过的数据执行dplyr的清洗
  lapply(function(df) dplyr::filter(df, Date != "Date") %>% # 因为有些.out文件中部还有变量名，所需需要用filter把这些行过滤掉
                      dplyr::mutate(Date = as.character(Date),Prac = as.character(Prac),
                                    Sub = as.numeric(Sub),Age = as.numeric(Age),Sex = as.character(Sex),Hand = as.character(Hand),
                                    Block = as.numeric(Block),Bin = as.numeric(Bin),Trial = as.numeric(Trial),
                                    Shape = as.character(Shape),Label = as.character(Shape),Match = as.character(Match),
                                    CorrResp = as.character(CorrResp),Resp = as.character(Resp),
                                    ACC = as.numeric(ACC),RT = as.numeric(RT)
                                    ) # 有些文件里读出来的数据格式不同，在这里统一所有out文件中的数据格式成字符串类型或数值型
         ) %>%
  bind_rows()
```

最终，我们得到一个包含所有数据框的列表，这些数据框都经过了一系列的转换和操作，然后使用“bind_rows”将它们合并成一个数据框。
虽然这个过程看起来很冗长，但思路清晰。最终，我们得到一个包含16个变量和25000多个观测值的数据框。

我们读取完后，可以将读取后的数据保存一下，以便下次直接读取。
我们前面整理并合并后的数据框需要一个名字和路径。我们可以使用相对路径，在当前工作目录下的data/match文件夹中将其保存为match_row.csv。一个常用的参数是强制将行名写入文件中，即row.names等于FALSE。

```{r write.csv}
#for loop 或 lapply的都可以
write.csv(df.mt.out.fl, file = "./data/match/match_raw.csv",row.names = FALSE)
#write.csv(df.mt.out.la, file = "./data/match/match_raw.csv",row.names = FALSE)
```

## 数据预处理

假设我们已经准备好了match和penguin的数据，我们可以读取数据并开始数据预处理。我们使用刚才保存的csv文件，使用header和sep参数设定来处理数据。

在tidyverse中，filter和mutate是常用的功能之一。group_by可以根据某些变量对数据进行分组，但一定要记得使用ungroup。
使用summarise可以计算均值、标准差、标准误等统计量。将group_by和summarise结合起来，我们可以快速有效地得到心理学中常用的统计量，如均值、标准差、标准误等。
我们可以对数据进行分组和条件筛选。“select”函数是用来选择列，与“filter”函数选择行不同。有时我们也可以用“select”函数重新排序，而“arrange”函数则是用来对整个数据框按某一列的值进行排序。

接下来我们来看一个例子，假设我们要选择1995年或之后出生的人，我们可以在管道中使用“filter”函数。在“dplyr”中，函数的参数是有顺序的，第一个是数据框。我们可以用.来代表输入数据，然后用“age”作为筛选条件。筛选完后，我们可以把结果输入到下一个函数中。如果要保留结果，我们需要把它赋值到一个新的变量中。

```{r example of select rawdata_penguin}
# 读取原始数据
df.pg.raw <-  read.csv('./data/penguin/penguin_rawdata.csv',
                       header = T, sep=",", stringsAsFactors = FALSE)
# 在数据框pg_raw中选择变量，使用select选择age和ALEX开头的所有题目
df.clean.select <- df.pg.raw %>%
  dplyr::select(age, starts_with("ALEX"), eatdrink, avoidance)
#笨一点的方法，就是把16个ALEX都写出来
```
```{r example of select rawdata_penguin DT, echo=FALSE}
# 看看其他变量是不是都消失了
DT::datatable(head(df.clean.select, 10),
              fillContainer = TRUE, options = list(pageLength = 3))
```

starts_with是tidyverse中的一个包，可以方便地选择以“ALEX”开头的所有列。它本质上是一个简化的通配符，因为在dplyr中，我们经常需要选择以某个特定开头或结尾的列，如果每次都写通配符会很麻烦。使用starts_with包可以直接选择以“ALEX”开头的列。

使用mutate函数可以生成一个新的变量，不仅仅是求和，还可以进行任意转换，比如加减乘除或判断。在这里，我们使用mutate函数将前四个“ALEX”列的得分求和，得到一个新的变量“ALEX_SUM”，表示前四个“ALEX”列的得分总和。

```{r example of mutate_1 rawdata_penguin}
# 把ALEX1 - 4求和
df.clean.mutate_1 <- df.pg.raw %>% 
  dplyr::mutate(ALEX_SUM = ALEX1 + ALEX2 + ALEX3 + ALEX4)
```
```{r example of mutate_1 rawdata_penguin DT, echo=FALSE}
# 看看是不是真的求和了
DT::datatable(head(df.clean.mutate_1, 10),
              fillContainer = TRUE, options = list(pageLength = 3))
```

需要注意的是，它是逐行运算，可以使用其他函数如rowSums，其中也用到了通配符。我们可以利用dplyr中的筛选功能，选取所有以＂ALEX＂为开头的列，并对这些列进行逐行求和，以得到真正反映Alex所有项目的总和。这种方法将给出16个项目的总和，而前面提到的方法只有4个项目的总和。

```{r example of mutate_2 rawdata_penguin}
# 对所有含有ALEX的列求和
df.clean.mutate_2 <- df.pg.raw %>% 
  dplyr::mutate(ALEX_SUM = rowSums(select(., starts_with("ALEX"))))
```
```{r example of mutate_2 rawdata_penguin DT, echo=FALSE}
DT::datatable(head(df.clean.mutate_2, 10),
              fillContainer = TRUE, options = list(pageLength = 3))
```

此外，我们还可以使用mutate函数对数据进行重新编码，例如根据出生年龄将其分成不同的年龄段。我们可以使用case_when函数生成一个新变量，该变量根据条件将列的内容变为不同的值。这种方法可以用于反向编码，例如将原来等于1的值变为5，将原来等于2的值变为4等。

```{r example of mutate_3 rawdata_penguin}
df.clean.mutate_3 <- df.pg.raw %>% 
  dplyr::mutate(decade = case_when(age <= 1969 ~ 60,
                                   age >= 1970 & age <= 1979 ~ 70,
                                   age >= 1980 & age <= 1989 ~ 80,
                                   age >= 1990 & age <= 1999 ~ 90,
                                   TRUE ~ NA_real_)  # 后面要跟一个TRUE，防止还存在没有穷尽的情况。表示其他所有上述范围的条件都是真的，它们被赋为NA
                ) %>% #当括号多的时候注意括号的位置 
  dplyr::select(.,decade, everything())
```
```{r example of mutate_3 rawdata_penguin DT, echo=FALSE}
DT::datatable(head(df.clean.mutate_3, 10),
              fillContainer = TRUE, options = list(pageLength = 3))
```

我们可以先按照出生年代将数据进行分组，然后使用group_by函数重新分组。我们还可以按照多个条件进行分组，例如按照年代和性别进行分组。分组完成后，我们可以使用summarise函数对每个组内部进行操作，例如求均值和标准差等。最后，我们可以使用ungroup函数将数据重新拆分，以便进行后续的运算。

```{r example of group_by rawdata_penguin}
df.clean.group_by <- df.clean.mutate_3 %>%
  dplyr::group_by(.,decade) %>% # 根据被试的出生年代，将数据拆分
  dplyr::summarise(mean_avoidance = mean(avoidance)) %>% # 计算不同年代下被试的平均avoidance
  dplyr::ungroup() #group之后一定要ungroup，否则分组的标签会一直在数据框上
```
```{r example of group_by rawdata_penguin DT, echo=FALSE}
# 拆分文件并不会让源文件产生任何视觉上的变化
DT::datatable(head(df.clean.group_by, 4),
              fillContainer = TRUE, options = list(pageLength = 4))
```

我们可以将所有学到的函数串起来
1.先使用filter函数选择eatdrink为1的被试
2.使用select函数选择所需变量
3.使用mutate函数对出生年份进行重新编码
4.使用group_by和summarise函数求出按照年代求ALEX的均值
5.使用rowSums函数对ALEX求均值。

```{r example of total rawdata_penguin}
df.pg.clean <- df.pg.raw %>%
  dplyr::filter(eatdrink == 1) %>% # 选择eatdrink为1的被试
  dplyr::select(age, starts_with("ALEX"), eatdrink, avoidance) %>%
  dplyr::mutate(ALEX1 = case_when(ALEX1 == '1' ~ '5', # 反向计分
                                  ALEX1 == '2' ~ '4',
                                  ALEX1 == '3' ~ '3',
                                  ALEX1 == '4' ~ '2',
                                  ALEX1 == '5' ~ '1',
                                  TRUE ~ as.character(ALEX1))) %>%
  dplyr::mutate(ALEX1 = as.numeric(ALEX1)) %>%
  dplyr::mutate(ALEX_SUM = rowSums(select(., starts_with("ALEX"))), # 把所有ALEX的题目分数求和
                decade = case_when(age <= 1969 ~ 60, # 把出生年份转换为年代
                                   age >= 1970 & age <= 1979 ~ 70,
                                   age >= 1980 & age <= 1989 ~ 80,
                                   age >= 1990 & age <= 1999 ~ 90,
                                   TRUE ~ NA_real_)) %>%
  dplyr::group_by(decade) %>% # 按照年代将数据拆分
  dplyr::summarise(mean_ALEX = mean(ALEX_SUM)) %>% # 计算每个年代的被试的平均的ALEX_SUM
  dplyr::ungroup() # 解除对数据的拆分
```

在这里，我们可以按照多个条件进行分组，但我们只使用了一个条件，因此可以写得很简洁。我们使用summarise函数计算ALEX总分的平均值，然后取消分组。我们可以得到每个十年在ALEX得分上的平均值。

在这个过程中，我们需要理解管道的操作方式，因为它可以处理我们想要对某个变量的所有领域进行的所有操作。每个函数都可以单独使用，但将它们放入管道中可以省略一些步骤。

在Tidyverse中，有一些常用的实用函数，比如separate、extract、unite和pivot。
separate可以按照特定规则将一个变量分割成为几列。它更适合用于按固定分隔符分割字符串，如将日期“2022-02-25”按照年月日分成“2022”、“02”和“25”三列
extract可以提取一个或多个特定的字符串。它更适合用于从字符串中提取特定的信息，如将“John Smith”分成“John”和“Smith”两列
unite则是将多个列合并成为一列。
pivot则可以将宽格式数据转换为长格式数据，或者将长格式数据转换为宽格式数据。

**长数据和宽数据**

那么，什么是长格式数据和宽格式数据呢？
我们通常使用长格式数据来记录实验数据，比如eprime产生的数据。长格式数据的形式可能是这样的：每个实验对象有一个ID，然后有不同的条件和变量，比如年龄和性别。每个实验对象可能有多个条件，因此我们需要以行来记录数据。每一行代表一个**观察值**。
例如，如果我们有一个问卷调查，其中有五个问题，那么我们将每个问题的回答记录在一列中，每一行代表受访者的一个回答值。
相反，宽格式数据是指我们将变量记录在多列中，每一行代表一个观察值。例如，如果我们有一个实验，其中有五个条件，那么我们将每个条件的结果记录在一列中，每一行代表一个受试者。

> 长数据和宽数据是数据分析和处理中常用的两种不同的数据结构。
>
> 长数据通常表示为一列数据包含多个变量，每个变量在不同的行中重复出现。例如，一列包含所有参与者的年龄、性别和教育水平，每个参与者在数据集中有多行记录。长数据集通常适用于需要进行聚合和统计分析的情况。
>
> 宽数据则通常表示为多列数据包含多个变量，每个变量在同一行中出现。例如，一列包含参与者的年龄，另一列包含性别，第三列包含教育水平。每个参与者只有一行记录。宽数据集通常适用于需要进行分组和比较分析的情况。
>
> 两种数据结构都有其优缺点，具体使用哪种结构取决于数据的分析目的和方法。
>
> 当一个数据集中的每一行都是单个观察单位时，通常使用宽数据格式。下面是一些宽数据格式的例子：
>
> - 一份包含人口统计数据的电子表格，每一行表示一个城市或地区，每一列则包含不同的人口统计数据，如总人口数、平均年龄、平均收入等等。
> - 一个电商网站的订单数据集，每一行表示一个订单，每一列包含订单的属性，如订单号、购买日期、购买者姓名、商品名称、数量、价格等等。
> - 一个医疗研究的数据集，每一行表示一个受试者，每一列包含该受试者的不同测量结果，如身高、体重、血压、血糖等等。
>
> 当数据集中的每一行表示的是一个观察单位的不同取值时，通常使用长数据格式。下面是一些长数据格式的例子：
>
> - 一份学生考试成绩的数据集，每一行表示一个学生的一门考试成绩，每一列包含学生的属性（如学生ID、姓名、年级、学科等）和考试成绩。
> - 一个心理学实验的数据集，每一行表示一个受试者在实验中的一次操作，每一列包含操作的属性（如受试者ID、实验条件、操作类型等）和操作结果。
> - 一份股票市场的数据集，每一行表示某支股票在某个时间点的市场数据，每一列包含市场数据的属性（如股票代码、日期、开盘价、收盘价、成交量等）。

## 函数

### 函数参数
学到这里，为了让大家更好的理解函数的意义和作用，在这里重新讲解一下一些常用的函数。

read.csv()的功能是将数据读取，成为R里面能够操纵的一个变量。read.csv就是我们的函数名，括号里面放置的是我们要输入的参数argument。
第一个argument是一个文件的路径，第二个是header，表示是否使用第一行作为我们的column names，第三个是sep，表示我们指定的分隔符是什么。还有第四个，force，表示是否把我们读到的文件里面的字符串作为factor。每一个函数基本上都包含两个部分，即**函数名和arguments**。

```{-}
function1(argument1 = 123,argument2 = "helloworld",argument3 = list1) 
#第一种输入方法：同时输入argument和value

function2(123,"helloworld",list1) 
function2("helloworld",list1) 
#二一种输入方法：也可以省略argument,顺序读取value
function2("helloworld",123,list1) 
```

我们有几种输入argument的方法，第一种是完整的输入，即function name和argument都要输入。第二个方法就是省略arguement name，直接输入数值，如果我们只给出了值，而没有明确指定参数，那么函数会按照顺序把值赋给参数。如果我们只给出了一个值，那么函数会默认把它赋给第一个参数，而其他参数则会有默认值。
对于read.csv()函数，除了第一个argument（文件名）以外，其余argument都有默认值。例如，header默认为false，separate默认为空格，quote为一个特殊符号表示引号。另外，stringsAsFactors也是一个默认值。
因此，我们只需要输入文件名这一个argument即可。如果不输入其他argument，它会使用默认值。这是R的一个重要特点，它兼具灵活性和便捷性。如果我们要完整写出来，我们需要写出文件名这个argument的名字，即file。我们可以输入相对路径或绝对路径。其他argument的默认值可以通过？或者help查看。

### group_by()

group_by有两个作用：第一个是分组，然后将其拆分为不同的小包。在对分组后的数据进行分析时，它将以组为单位进行操作，对每个组的结果分别计算，然后组合起来。如果我们在没有任何操作的情况下添加groupby，它会如何改变我们的数据框呢？

```{r}
# 读取原始数据
df.mt.raw <-  read.csv('./data/match/match_raw.csv',
                       header = T, sep=",", stringsAsFactors = FALSE) 
library("tidyverse")
df.mt.raw
group <- df.mt.raw %>% 
  group_by(Shape)
group#注意看数据框的第二行，有Groups:   Shape [4]的信息
```

对比一下，经过了group_by后的数据框，增加了一个Groups:Shape [4]的提示，它有一个标记，表示它已经在内部进行了分组，分组标准是shape，即形状。如果想删除该shape变量，就必须ungroup。否则，这个分组标签将一直存在于数据框中。因此，我们建议在进行groupby之后一定要进行ungroup，否则分组标签将一直存在于数据框中。

实际上，我们不会像现在这样无聊地直接添加group_by，然后看它会发生什么。我们需要明确后面分析的逻辑是什么。我们可以通过group_by将数据框按照base的ID分成几个小的数据框，然后以每个亚组为单位进行计算。比如，我们可以用summarise求出每个subgroup的行数，然后返回到n里面去，得到一个描述性的结果。但是，我们需要注意，当我们得到新的结果后，一定要把它ungroup掉，否则会影响后面的分析。此外，我们需要注意，如果我们再进行一个group_by，而没有ungroup前面的结果，可能会覆盖掉前面的结果，这是需要注意的问题。

如果我们不使用group_by，我们以前的做法是先申请一个中间变量，然后将数据按条件分组，选出一个subset出来，然后对每个操作分别求均值和标准差，最后将它们合并起来。现在有了group_by和summarise，我们可以在管道中一次性完成所有操作，而不需要生成大量的中间变量。这样做的好处是逻辑更清晰，代码更简洁，而且不会占用过多的内存
对于group_by()函数的作用，我们可以对比不使用它的效果。

```{r}
library("tidyverse")
df.mt.raw <-  read.csv('./data/match/match_raw.csv',
                       header = T, sep=",", stringsAsFactors = FALSE) 
group <- df.mt.raw %>% 
  group_by(.,Shape) %>% 
  summarise(n())
DT::datatable(group)

ungroup <- df.mt.raw %>% 
  summarise(n())
DT::datatable(ungroup)
```

### 定义函数

因为我们已经讲了函数的逻辑，即函数名、参数和操作。我们可以最简单的方式就是建立一个函数。我们用function来定义函数，它本身也是一个函数，用于帮助我们定义一个函数。我们可以在括号()中输入任何我们想定义的参数。然后在大括号{}中定义函数的操作，例如求和和乘积。

```{r}
fun1 <- function(a = 1,b = 100){
    sum <- 0
	for (i in a:b) {
  		sum <- sum + i
	}
    return(sum)
}
```

在这个函数中，a和b都有默认值，a的默认值是1，b的默认值是100。如果我们不给它们赋值，它们就会按照默认值运行。我们可以给a和b赋值，然后运行函数，得到sum的值。这种方式非常有用，因为有时候现有的函数可能不够满足我们的需求，我们可以自己定义一个简单的函数来处理数据。我们可以定义一个函数来进行实验操作和数据分析，这样可以方便后续的优化和重复使用。

在R中，括号有不同的用法，中括号一般用于调用变量中的数据，根据变量的不同特点，可能需要输入不同的index值。索引某个变量中的元素通常使用中括号，小括号通常用作函数的参数或输入。在函数中，小括号表示输入参数的值；大括号一般表示为一个代码块。if else语句中，小括号表示输入判断条件。在for循环中，我们也需要使用小括号和大括号来输入循环条件和操作。

```{r}
fun1 <- function(a = 1,b = 100){#第一个{构成最外层代码块的起始
    sum <- 0
	for (i in a:b) {#第二个{构成次外层代码块的起始
  		sum <- sum + i
	}#与第二个{对应的}构成次外层代码块的结尾
    return(sum)
}#与第一个{对应的}构成最外层代码块的结尾
```

### for loop

**for (variable in sequence) {statement}**

for loop是一种循环语句，它的语法结构是for (variable in sequence) {statement}，其中variable是一个代符，sequence是一个向量或列表，而statement则是要重复执行的某一个语句。在每一次循环中，variable都被赋予为sequence里面的一个元素，然后执行一次statement，再回到sequence里面的下一个元素，不断地循环。每一次循环，variable都会变成sequence里的下一个元素，因此它只是一直在变化。

```{r}
sum <- 0
#variable i  sequence 1:100
for (i in 1:100) {#statement
  sum <- sum + i
}
print(sum)
```

举个例子，如果我们要对1到100的数据进行求和，我们可以定义一个sum变量，然后用for loop来实现。在这个例子中，i是variable，1:100是sequence，1到100的所有整数是1:100里面的元素。每一次循环，sum都会加上i的值，最终得到1到100的和。在第一次循环时，sum的初始值为0，i的值为1，所以sum会更新为1。在第二次循环时，i的值为2，sum的值为1，所以sum会更新为3。以此类推，直到循环结束，sum的值就是1到100的和。

```{r}
files <- list.files(file.path("data/match"), 
                    pattern = "data_exp7_rep_match_.*\\.out$")

df_list <- list()

for (i in seq_along(files)) {
  df <- read.table(file.path("data/match", files[i]), header = TRUE) 
  df_list[[i]] <- df
}
```

在批量读取文件的for循环中，我们读取了所有以data_exp7开头，以out结尾的文件，使用list.files函数匹配符合模式的所有文件，得到一列文件名list，然后循环list。我们需要对1到num files进行操作，这个sequence是从1开始的。在R和MatLab中，我们也是从1开始的，但在Python中，我们是从0开始的。

在for循环中，我们使用i in seq_along(files)来循环操作，从i等于1开始，然后一直到最后一个元素。当i等于1时，我们使用read table函数，并从files中提取出第i个元素，这个元素实际上是一个字符，也就是代表某一个特定的文件。

file path函数将前面的文件夹和后面的文件名整合成一个完整的文件路径。因此，read table函数的第一个argument就是这个file path。当进行了读取操作之后。这个df实际上就是第一个out文件的数据。

在datalist中，i代表第几个元素，使用df.list[i]来index list。在第一次循环中，我们将df给到df.list的第一个元素。

因此，当i等于1时，我们开始第一次循环，首先有一个files代表需要操作的文件。比方说xxxxxx.out，对吧？第一个元素是什么，第二个元素也是什么.out，一直到最后一个元素。当我们i=1的时候，我们把它的第一个元素拿出来，然后把它拿到read.table里面去。这个filepath本身又是一个函数，我们把这个第一个文件夹的名字和第一个out文件路径加起来，就是file.filepath这个地方。然后再开启read.table的第二个argument，那就是header=true。通过这个for loop，我们读取了所有的文件，把读取的结果复制到了df.list里面的对应元素。这个时候，df_list就更新了。以此类推直到所有的files里面的所有数据都读取完，那么这个时候我们的df_list就是一个包含了所有数据的一个list，每一个元素代表一个base的一个数据。

### 信号检测

接下来，我们回顾上一节课的练习题。我们使用数据预处理的方法，求出match数据中的d prime，这是一个信号检测论的指标，也称为敏感性指标。

我们可以将数据分为signal和noise，其中signal是指hit，对应的是correct rejection；而如果刺激中有noise，我们认为它是有信号的，这就是FA。如果我们报告时认为刺激中有信号，但实际上是noise，这就是Miss。根据信号检测论，我们将刺激分为match和mismatch，其中match是信号，而mismatch或nonmatch是noise。我们的反应是指我们呈现的刺激。如果反应是match，那么这就是hit。对于match的trial，如果我们的反应是mismatch，那么这就是信号减速论的miss。正确率是0，因为它是错误的。对于mismatch的trial，如果我们的反应是mismatch，那么这就是CR，正确率是1。FA的正确率是0。我们需要计算d'，它等于zhit减去zFA，这是我们敏感性的指标。我们需要用R来计算每个条件下的d'，因此我们需要使用group_by。最终，我们将得到一个比例，其中包括base的id、条件和d'。

我们在实际使用这些函数时，是带着目的去使用的。想要计算击中率虚报率，我们首先要知道什么情况是击中，也要让计算机知道什么情况是击中，一旦分类了，计算的时候也需要进行判断。但在分类判断之前，我们要先告诉计算机什么是正确情况什么是错误情况，哪些是这个被试做的，哪些不是，所以也需要分组。

我们有了基本的思路：1.分组，使用group_by。2.告诉计算机什么是hit，什么是miss，使用summarise函数。3.利用判断语句进行分类计算，我们这里使用了ifelse。

我们选出需要的几列，包括自变量和因变量，以及分组变量。使用select()函数，因为数据中存在缺失值，所以也需要提剔除它们。

```{r}
df.mt.clean <- df.mt.raw %>%
  dplyr::select(Sub, Block, Bin,  # block and bin
                Shape, Match, # 自变量
                ACC, RT, # 反应结果
                ) %>% 
  tidyr::drop_na()
```

在计算击中率误报率等的时候，我们针对的是被试的在某一特定实验条件下的反应。所以我们需要通过bin、block、shape、sub进行分组，这样进行计算时，就是每一个条件组下分别计算。不会出现在虚报的实验条件下计算正确拒绝的情况。

```{r}
df.mt.clean <- df.mt.raw %>%
  dplyr::select(Sub, Block, Bin,  # block and bin
                Shape, Match, # 自变量
                ACC, RT, # 反应结果
                ) %>% 
  tidyr::drop_na()  %>% #删除缺失值
  dplyr::group_by(Sub, Block, Bin, Shape)
```

接下来就是要使用summarise函数来给击中、虚报等分类，并使用ifelse函数根据分类计算概率

```{r}
df.mt.clean <- df.mt.raw %>%
  dplyr::select(Sub, Block, Bin,  # block and bin
                Shape, Match, # 自变量
                ACC, RT, # 反应结果
                ) %>% 
  tidyr::drop_na()  %>% #删除缺失值
  dplyr::group_by(Sub, Block, Bin, Shape) %>%
  dplyr::summarise(
      hit = length(ACC[Match == "match" & ACC == 1]),
      fa = length(ACC[Match == "mismatch" & ACC == 0]),
      miss = length(ACC[Match == "match" & ACC == 0]),
      cr = length(ACC[Match == "mismatch" & ACC == 1]),
      Dprime = qnorm(
        ifelse(hit / (hit + miss) < 1,
               hit / (hit + miss),
               1 - 1 / (2 * (hit + miss))
              )
           ) - qnorm(
        ifelse(fa / (fa + cr) > 0,
               fa / (fa + cr),
               1 / (2 * (fa + cr))
              )
                    ))
```

在这里，我们使用了一个num来判断它们的数量。我们可以使用nrow代替。这只是其中一种做法。接下来，我们看一下ACC里面所有等于match的情况。ACC等于什么呢？条件是match，然后ACC等于1。当我们想要计算hit时，我们需要将两个条件进行组合。一个是刺激呈现的内容，另一个是正确率。只有在match条件下反应正确的试次才是hit的试次。通过这个length，我们计算出了在每一个base、每一个block、每个bin下面以及每一个条件shape之下有多少个hit的比值。

同样的逻辑，我们通过mismatch和0计算出了FA（false alarm）。如果match的accuracy是0，我们就错失了这个信息，也就是miss。这是我们前面知识的一个简单应用。

```{r}
df.mt.clean <- df.mt.raw %>%
  dplyr::select(Sub, Block, Bin,  # block and bin
                Shape, Match, # 自变量
                ACC, RT, # 反应结果
                ) %>% 
  tidyr::drop_na()  %>% #删除缺失值
  dplyr::group_by(Sub, Block, Bin, Shape) %>%
  dplyr::summarise(
      hit = length(ACC[Match == "match" & ACC == 1]),
      fa = length(ACC[Match == "mismatch" & ACC == 0]),
      miss = length(ACC[Match == "match" & ACC == 0]),
      cr = length(ACC[Match == "mismatch" & ACC == 1]),
      Dprime = qnorm(
        ifelse(hit / (hit + miss) < 1,
               hit / (hit + miss),
               1 - 1 / (2 * (hit + miss))
              )
           ) - qnorm(
        ifelse(fa / (fa + cr) > 0,
               fa / (fa + cr),
               1 / (2 * (fa + cr))
              )
                    )) %>% 
  dplyr::ungroup() %>%
  select(-"hit",-"fa",-"miss",-"cr") %>%
  dplyr::group_by(Sub, Shape)  %>%
  tidyr::pivot_wider(names_from = Shape,
                     values_from = Dprime) 
```

然后，我们计算出了每一个d'。我们可以看到，这个d'是用qnorm计算的。hit的比值是等于hit除以hit加miss。这个地方有点复杂，它实际上就是想要把这两个东西打包在一个语句里面。zheat和zfa有两个条件，首先我们看它的两个部分，一个是qnorm，表示hit的部分，另一个是FA部分。我们计算出hit rate，然后将其转换成z分数，减去quantum。这个地方为什么要用if else呢？因为会出现一个比较特殊的情况，比方说也许某个被试的hit全部是正确的，即正确率为1，此时再用qnorm的话，结果是一个无限大的数，就是一个正向的infinity。对这种可能的特殊情况，我们需要把hit rate转换成一个小于1的值，来避免出现infinity。因为我们无法对infinity进行进一步的计算。所以，如果hit rate小于1，我们就用它。

这个是if else在这个type里面的应用。如果前面hit rate小于1，那么我们就使用它自己的值。如果它不小于1，也就是等于1，那么我们就用1减去1除以括号里的值，这样可以使得结果变得稍微小一点。这是一个常用的方法，在信号检测论里面也常用。对于FA也是同样的方法，我们使用if else，如果它大于0，那么我们就用它；如果等于0，那么我们就需要想一个办法让它变成一个不是0的值，来避免得到一个负向无穷大的结果，这样就无法进行后续的运算。接下来我们就可以算出d'，然后去掉后面所有的hit、fn、miss和cr，进行后续的运算。这个逻辑清晰吗？

我们可以拆开每一个函数来看，选择columns group by，summarise group by，qvolume是一个把百分比转换成为1分数的函数，这个大家需要去后面查找。然后我们进行相减，插入条件语句，再把它转换成1分数，最后就得到d'。我们也可以进行后续的运算，再次group_by，subject和shape。我们可以查看每一个被试在每一个bin上面的结果的变化。如果我们收了很多被试，可能就不需要关注每一个被试的变化，只需要报告总体的结果即可。我们可以根据time inversion里面的数据预数的一系列操作，一个一个管道下来，最后直接得到我们想要的结果。这样的管道操作可能需要不断迭代，刚开始可能只会做最简单的预处理，但是迭代到最后，就会形成一个很长的管道。

数据处理的流程非常清晰。在这个过程中，我们对ACC进行了选择，相当于是选择了符合条件的一些行，并求出它的长度。虽然理论上讲，我们也可以用别的变量来代替ACC，比如RT，但是ACC是最方便的选择，因为我们后面也会用到它。但是，这并不是唯一的操作，我们还可以用其他的变量来进行操作，比如length换成n。如果我们用filter的话，也是可行的，但可能需要写更多的代码。我们要根据自己的研究目的或者想要操作的目的去进行数据处理，把各种预处理操作进行组合。
